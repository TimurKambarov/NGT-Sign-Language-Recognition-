"""
NGT Sign Language Recognition - V5 (FULL HAND COMPARISON)
=========================================================
Compares ALL 21 landmarks throughout the gesture for precise recognition.

Why this is better:
- Tracks the ENTIRE hand, not just one finger
- Random gestures won't match because ALL fingers must be in right positions
- More precise and accurate

Controls:
    SPACE = Start/Stop recording
    Q = Quit
    M = Toggle mirror
    D = Toggle debug info
"""

import cv2
import mediapipe as mp
import numpy as np
import os
from scipy.spatial.distance import euclidean

try:
    from fastdtw import fastdtw
except ImportError:
    print("ERROR: Install fastdtw: pip install fastdtw")
    exit()

# ============== CONFIGURATION ==============

DATA_DIR = "data/dynamic"
MIN_FRAMES = 15
MAX_FRAMES = 150

# Recognition thresholds
DTW_THRESHOLD = 35.0          # Threshold for full hand comparison (higher because more data)
MIN_CONFIDENCE = 0.90         # STRICT: Must be 90%+ confident to recognize
MOVEMENT_THRESHOLD = 0.03     # Minimum movement required

# ============== MEDIAPIPE SETUP ==============

mp_hands = mp.solutions.hands
mp_drawing = mp.solutions.drawing_utils

hands = mp_hands.Hands(
    static_image_mode=False,
    max_num_hands=1,
    model_complexity=1,
    min_detection_confidence=0.4,
    min_tracking_confidence=0.4
)

# ============== HELPER FUNCTIONS ==============

def extract_landmarks(hand_landmarks):
    """
    Extract ALL landmark values:
    - 3 values: Absolute wrist position (x, y, z)
    - 63 values: All 21 landmarks relative to wrist
    = 66 total values
    
    This captures:
    - Hand SHAPE (fingers relative to wrist)
    - Hand POSITION (absolute wrist for H bounce detection)
    """
    landmarks = []
    wrist = hand_landmarks.landmark[0]
    
    # First: Absolute wrist position (for H bounce)
    landmarks.extend([wrist.x, wrist.y, wrist.z])
    
    # Then: All landmarks relative to wrist (hand shape)
    for lm in hand_landmarks.landmark:
        landmarks.extend([
            lm.x - wrist.x,
            lm.y - wrist.y,
            lm.z - wrist.z
        ])
    
    return np.array(landmarks, dtype=np.float32)  # 66 values


def calculate_movement(frames):
    """Calculate total movement amount across all landmarks"""
    if len(frames) < 2:
        return 0
    
    frames_arr = np.array(frames)
    
    # Calculate frame-to-frame differences
    diffs = np.diff(frames_arr, axis=0)
    
    # Sum of all movement
    total_movement = np.sum(np.abs(diffs))
    
    # Normalize by number of frames
    return total_movement / len(frames)


def normalize_sequence(frames):
    """
    Normalize a sequence for comparison:
    1. Center around mean position
    2. Scale to unit variance
    """
    frames_arr = np.array(frames)
    
    # Center
    mean = np.mean(frames_arr, axis=0)
    centered = frames_arr - mean
    
    # Scale
    std = np.std(centered)
    if std > 0.001:
        normalized = centered / std
    else:
        normalized = centered
    
    return normalized


def load_templates():
    """Load all recorded templates"""
    templates = {'H': [], 'J': [], 'Z': []}
    
    for letter in templates.keys():
        letter_dir = os.path.join(DATA_DIR, letter)
        if os.path.exists(letter_dir):
            files = [f for f in os.listdir(letter_dir) if f.endswith('.npy')]
            for f in files:
                data = np.load(os.path.join(letter_dir, f))
                if len(data) >= MIN_FRAMES:
                    templates[letter].append(data)
    
    print("\nüìÇ Templates loaded:")
    total = 0
    for letter, temps in templates.items():
        print(f"   {letter}: {len(temps)} samples")
        total += len(temps)
    print(f"   Total: {total} samples")
    
    return templates


def compare_gestures(user_frames, template_frames):
    """
    Compare two gesture sequences using DTW on ALL landmarks.
    Returns normalized distance.
    """
    # Normalize both sequences
    user_norm = normalize_sequence(user_frames)
    template_norm = normalize_sequence(template_frames)
    
    # DTW comparison
    try:
        distance, _ = fastdtw(user_norm, template_norm, dist=euclidean)
        # Normalize by sequence lengths
        normalized_distance = distance / (len(user_norm) + len(template_norm))
        return normalized_distance
    except Exception as e:
        return float('inf')


def recognize_gesture(frames, templates):
    """
    Recognize gesture by comparing ALL landmarks against ALL templates.
    
    STRICT: Only returns a match if confidence >= 90%
    
    Returns: (letter, distance, confidence, details, reason)
    """
    # Check minimum movement
    movement = calculate_movement(frames)
    if movement < MOVEMENT_THRESHOLD:
        return None, 0, 0, {}, f"Not enough movement ({movement:.4f} < {MOVEMENT_THRESHOLD})"
    
    results = {}
    
    for letter, letter_templates in templates.items():
        if not letter_templates:
            results[letter] = {'distance': float('inf'), 'confidence': 0}
            continue
        
        # Compare with all templates for this letter, take best match
        best_distance = float('inf')
        
        for template in letter_templates:
            distance = compare_gestures(frames, template)
            if distance < best_distance:
                best_distance = distance
        
        # Calculate confidence
        if best_distance < DTW_THRESHOLD:
            confidence = 1 - (best_distance / DTW_THRESHOLD)
        else:
            confidence = 0
        
        results[letter] = {
            'distance': best_distance,
            'confidence': confidence
        }
    
    # Find best match
    best_letter = None
    best_distance = float('inf')
    best_confidence = 0
    
    for letter, data in results.items():
        if data['confidence'] > best_confidence:
            best_distance = data['distance']
            best_confidence = data['confidence']
            best_letter = letter
    
    # STRICT CHECK: Only accept if 90%+ confident
    if best_confidence >= MIN_CONFIDENCE:
        return best_letter, best_distance, best_confidence, results, "Match found"
    
    # Rejected - explain why
    if best_confidence > 0:
        return None, best_distance, best_confidence, results, f"Not confident enough ({best_confidence:.0%} < {MIN_CONFIDENCE:.0%})"
    
    return None, best_distance, 0, results, "No match found - gesture doesn't match any letter"


# ============== DRAWING ==============

def draw_hand(frame, hand_landmarks):
    h, w = frame.shape[:2]
    
    mp_drawing.draw_landmarks(frame, hand_landmarks, mp_hands.HAND_CONNECTIONS,
        mp_drawing.DrawingSpec(color=(0, 180, 0), thickness=2, circle_radius=2),
        mp_drawing.DrawingSpec(color=(0, 150, 0), thickness=2))
    
    # Color-coded fingertips
    fingertips = {
        4: (0, 165, 255),   # Thumb - Orange
        8: (0, 255, 0),     # Index - Green
        12: (255, 100, 0),  # Middle - Blue
        16: (0, 255, 255),  # Ring - Yellow
        20: (255, 0, 255),  # Pinky - Magenta
    }
    
    for tip_idx, color in fingertips.items():
        tip = hand_landmarks.landmark[tip_idx]
        x, y = int(tip.x * w), int(tip.y * h)
        cv2.circle(frame, (x, y), 10, color, -1)
        cv2.circle(frame, (x, y), 12, (255, 255, 255), 2)
    
    return frame


def draw_ui(frame, state, frames_count, result, confidence, debug_results, show_debug, message=""):
    h, w = frame.shape[:2]
    
    # Top panel
    cv2.rectangle(frame, (0, 0), (w, 100), (0, 0, 0), -1)
    
    if state == "WAITING":
        cv2.putText(frame, "Press SPACE to start recording", (20, 40),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 255), 2)
        cv2.putText(frame, "Do the COMPLETE gesture (H, J, or Z), then press SPACE to stop", (20, 75),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1)
    
    elif state == "RECORDING":
        flash = int(cv2.getTickCount() / cv2.getTickFrequency() * 4) % 2
        cv2.circle(frame, (30, 40), 15, (0, 0, 255) if flash else (0, 0, 180), -1)
        cv2.putText(frame, f"RECORDING ({frames_count} frames)", (55, 45),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 0, 255), 2)
        cv2.putText(frame, "Do the gesture, then press SPACE to stop", (20, 80),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1)
    
    elif state == "RESULT":
        if result:
            cv2.putText(frame, f"Recognized: {result} ({confidence:.0%})", (20, 45),
                       cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 255, 0), 2)
        else:
            cv2.putText(frame, "NOT RECOGNIZED", (20, 45),
                       cv2.FONT_HERSHEY_SIMPLEX, 1.0, (0, 100, 255), 2)
            if message:
                cv2.putText(frame, message, (20, 75),
                           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (150, 150, 150), 1)
        cv2.putText(frame, "Press SPACE to try again", (20, 90),
                   cv2.FONT_HERSHEY_SIMPLEX, 0.45, (200, 200, 200), 1)
    
    # Result display
    if state == "RESULT":
        if result:
            box_x, box_y = w//2 - 90, h//2 - 70
            cv2.rectangle(frame, (box_x, box_y), (box_x + 180, box_y + 140), (0, 100, 0), -1)
            cv2.rectangle(frame, (box_x, box_y), (box_x + 180, box_y + 140), (0, 255, 0), 4)
            cv2.putText(frame, result, (box_x + 50, box_y + 95), cv2.FONT_HERSHEY_SIMPLEX, 3, (0, 255, 0), 6)
            cv2.putText(frame, f"{confidence:.0%}", (box_x + 60, box_y + 125), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (200, 255, 200), 2)
        else:
            box_x, box_y = w//2 - 120, h//2 - 50
            cv2.rectangle(frame, (box_x, box_y), (box_x + 240, box_y + 100), (0, 0, 80), -1)
            cv2.rectangle(frame, (box_x, box_y), (box_x + 240, box_y + 100), (0, 0, 200), 3)
            cv2.putText(frame, "NOT", (box_x + 70, box_y + 40), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 100, 255), 2)
            cv2.putText(frame, "RECOGNIZED", (box_x + 25, box_y + 80), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 100, 255), 2)
    
    # Debug info
    if show_debug and debug_results:
        y = h - 140
        cv2.putText(frame, f"All Letters Comparison (threshold={DTW_THRESHOLD}, min_conf={MIN_CONFIDENCE:.0%}):", 
                   (20, y), cv2.FONT_HERSHEY_SIMPLEX, 0.45, (255, 255, 0), 1)
        
        for letter in ['H', 'J', 'Z']:
            y += 25
            if letter in debug_results:
                data = debug_results[letter]
                dist = data['distance']
                conf = data['confidence']
                
                if conf >= MIN_CONFIDENCE:
                    color = (0, 255, 0)  # Green - would be accepted
                    status = "‚úì MATCH"
                elif conf > 0:
                    color = (0, 200, 200)  # Yellow - close but not enough
                    status = "~ close"
                else:
                    color = (100, 100, 100)  # Gray - no match
                    status = "‚úó no match"
                
                text = f"{letter}: dist={dist:.2f}, conf={conf:.0%} {status}"
                cv2.putText(frame, text, (20, y), cv2.FONT_HERSHEY_SIMPLEX, 0.45, color, 1)
    
    # Controls
    cv2.rectangle(frame, (0, h - 30), (w, h), (30, 30, 30), -1)
    cv2.putText(frame, "SPACE=Record/Stop | Q=Quit | M=Mirror | D=Debug", (20, h - 10),
               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (150, 150, 150), 1)
    
    return frame


# ============== MAIN ==============

def main():
    print("\n" + "="*60)
    print("   NGT RECOGNITION V5 - FULL HAND COMPARISON")
    print("="*60)
    print("\n   This version compares ALL 21 hand landmarks + wrist!")
    print("\n   Why this is better:")
    print("   - Tracks entire hand, not just one finger")
    print("   - Includes wrist position (for H bounce)")
    print("   - Random gestures won't match (all fingers must be right)")
    print(f"\n   STRICT Settings:")
    print(f"   - Min Confidence: {MIN_CONFIDENCE:.0%} (must be 90%+ sure!)")
    print(f"   - DTW Threshold: {DTW_THRESHOLD}")
    print(f"   - Min Movement: {MOVEMENT_THRESHOLD}")
    print("\n   Controls:")
    print("   SPACE = Start/Stop | D = Debug | Q = Quit | M = Mirror")
    print("="*60 + "\n")
    
    templates = load_templates()
    
    if sum(len(t) for t in templates.values()) == 0:
        print("\n‚ùå No templates! Run: python record_simple.py")
        return
    
    cap = cv2.VideoCapture(0)
    cap.set(cv2.CAP_PROP_FRAME_WIDTH, 1280)
    cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 720)
    
    state = "WAITING"
    frames_buffer = []
    result = None
    confidence = 0
    debug_results = {}
    message = ""
    
    mirror_mode = True
    show_debug = False
    
    print("‚úÖ Ready! Press SPACE to record.\n")
    
    while True:
        ret, frame = cap.read()
        if not ret:
            break
        
        if mirror_mode:
            frame = cv2.flip(frame, 1)
        
        rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)
        hand_results = hands.process(rgb)
        
        hand_detected = False
        if hand_results.multi_hand_landmarks:
            hand_detected = True
            hand_lm = hand_results.multi_hand_landmarks[0]
            frame = draw_hand(frame, hand_lm)
            
            if state == "RECORDING":
                frames_buffer.append(extract_landmarks(hand_lm))
                if len(frames_buffer) >= MAX_FRAMES:
                    state = "WAITING"
                    print("‚ö†Ô∏è  Max frames reached!")
        
        h, w = frame.shape[:2]
        indicator_color = (0, 255, 0) if hand_detected else (0, 0, 255)
        cv2.circle(frame, (w - 30, 30), 12, indicator_color, -1)
        
        frame = draw_ui(frame, state, len(frames_buffer), result, confidence, debug_results, show_debug, message)
        
        cv2.imshow("NGT Recognition V5", frame)
        
        key = cv2.waitKey(1) & 0xFF
        
        if key == ord('q'):
            break
        elif key == ord('m'):
            mirror_mode = not mirror_mode
            print(f"Mirror: {'ON' if mirror_mode else 'OFF'}")
        elif key == ord('d'):
            show_debug = not show_debug
            print(f"Debug: {'ON' if show_debug else 'OFF'}")
        
        elif key == ord(' '):
            if state == "WAITING":
                if hand_detected:
                    state = "RECORDING"
                    frames_buffer = []
                    result = None
                    debug_results = {}
                    message = ""
                    print("\nüî¥ Recording - do the COMPLETE gesture!")
                else:
                    print("‚ö†Ô∏è  Show your hand first!")
            
            elif state == "RECORDING":
                print(f"‚èπÔ∏è  Stopped ({len(frames_buffer)} frames)")
                
                if len(frames_buffer) >= MIN_FRAMES:
                    result, distance, confidence, debug_results, message = recognize_gesture(frames_buffer, templates)
                    
                    if result:
                        print(f"‚úÖ Recognized: {result}")
                        print(f"   Distance: {distance:.2f}")
                        print(f"   Confidence: {confidence:.0%}")
                    else:
                        print(f"‚ùå Not recognized: {message}")
                    
                    # Print all comparisons
                    print("   All comparisons:")
                    for letter in ['H', 'J', 'Z']:
                        if letter in debug_results:
                            d = debug_results[letter]
                            print(f"   {letter}: dist={d['distance']:.2f}, conf={d['confidence']:.0%}")
                    
                    state = "RESULT"
                else:
                    print(f"‚ö†Ô∏è  Too short! Need {MIN_FRAMES}+ frames")
                    state = "WAITING"
            
            elif state == "RESULT":
                state = "WAITING"
                frames_buffer = []
                result = None
                debug_results = {}
                message = ""
    
    cap.release()
    cv2.destroyAllWindows()
    hands.close()
    print("\nGoodbye!")


if __name__ == "__main__":
    main()